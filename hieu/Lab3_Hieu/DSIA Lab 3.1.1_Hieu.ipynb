{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PopLE1ywNQsa"
   },
   "source": [
    "![alt text](https://i.imgur.com/1WaY7aA.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e45O_NedNQsd"
   },
   "source": [
    "# Lab 3.1.1 \n",
    "# *Data Wrangling and Munging with Pandas*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qr08YR1PNQsf"
   },
   "source": [
    "## Part 1: Wrangling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RH9yA6LINQsh"
   },
   "source": [
    "The term \"data wrangling\" is analogous to capturing wild horses and getting them into a fenced area; the horses are data and the fencing is your computer. The more common data wrangling tasks include:\n",
    "\n",
    "- reading flat files\n",
    "- reading Excel files\n",
    "- downloading from web pages\n",
    "  - csv\n",
    "  - html\n",
    "  - json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Znip5IF3NQsk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8G9iecYNQsr"
   },
   "source": [
    "*It is good practice to display the library version numbers for future reference:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GuNeJ6hlNQst"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy:  1.16.2\n",
      "Pandas:  0.24.2\n"
     ]
    }
   ],
   "source": [
    "print('Numpy: ', np.__version__)\n",
    "print('Pandas: ', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzTKAG1LNQsx"
   },
   "source": [
    "### CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RE2wYKPNNQsy"
   },
   "source": [
    "Below are three attempts to load the file \"bikeshare.csv\" into a DataFrame named `bikes`. Why are they wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZV2HMWarNQsz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  instant,dteday,season,yr,mnth,hr,holiday,weekd...\n",
      "1  1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,...\n",
      "2  2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0...\n",
      "3  3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0...\n",
      "4  4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,...\n",
      "\n",
      "  1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n",
      "0  2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0...     \n",
      "1  3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0...     \n",
      "2  4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,...     \n",
      "3  5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,...     \n",
      "4  6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,...     \n",
      "\n",
      "  instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n",
      "0  1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,...                                                                   \n",
      "1  2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0...                                                                   \n",
      "2  3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0...                                                                   \n",
      "3  4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,...                                                                   \n",
      "4  5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,...                                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  import sys\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# wrong:\n",
    "bikes = pd.read_table('DATA3/bikeshare.csv', header = None)\n",
    "print(bikes.head())\n",
    "print()\n",
    "\n",
    "# wrong:\n",
    "bikes = pd.read_table('DATA3/bikeshare.csv', header = 1)\n",
    "print(bikes.head())\n",
    "print()\n",
    "\n",
    "# wrong:\n",
    "bikes = pd.read_table('DATA3/bikeshare.csv', header = 0)\n",
    "print(bikes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSiMJN9NNQs4"
   },
   "source": [
    "?:\n",
    "ANSWER: Case 1 treats headings as just another data row. Case 2 treats the 1st data row as the column header. Case 3 gets the header right (row 0), but reads each row as a single column (Nb. the other two make that same mistake). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDFYVoPINQs6"
   },
   "source": [
    "Load the file \"bikeshare.csv\" into a DataFrame named `bikes`, and confirm that it was loaded properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wH2wuPznNQs8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n"
     ]
    }
   ],
   "source": [
    "#ANSWER:\n",
    "bikes = pd.read_csv('DATA3/bikeshare.csv', header = 0, sep = ',')\n",
    "print(bikes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEwczD29NQtA"
   },
   "source": [
    "Note that we could have used `read.csv()` above. When is `read_table()` necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OsmNWLXNQtB"
   },
   "source": [
    "?:\n",
    "ANSWER: When `sep` is not the comma character, or we need fine control that `read.csv()` does not provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPEoeHGGNQtC"
   },
   "source": [
    "Flat files can be full of surprises. Here are some issues to watch out for:\n",
    "\n",
    "- separator character is something other than the comma\n",
    "  - \";\", \"|\", and tab are popular\n",
    "- newline character is something other than what the O/S expects \n",
    "  - Tip: Don't hard-code the character codes for carriage returns, linefeeds, etc. Use Python's built-in representation instead (e.g. Python translates \"\\n\" to the newline character and \"\\t\" to the tab character on any O/S).\n",
    "- truncated lines\n",
    "  - if there are empty fields at the end of a line it is possible that their separators will be missing, resulting in a \"jagged\" file\n",
    "- embedded commas or quotes\n",
    "  - a free-text field containing embedded commas may split into separate fields on input\n",
    "  - a free-text field containing embedded quotes may not parse correctly\n",
    "- unescaped characters\n",
    "  - the \"\\\" character indicates a control code to Python, which will break the I/O\n",
    "    - e.g. the substring \"\\u0123\" will be interpreted as Unicode(0123) -- which may not be what the file creator intended\n",
    "  - these may need to be fixed by loading whole strings and then parsing into a new data frame\n",
    "  \n",
    "Tip: Most issues can be delth with by correctly specifying the parameters of the function you use to load the file. Read the doco before reading the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0G5PtA2NQtC"
   },
   "source": [
    "### Reading Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYELgCw6NQtD"
   },
   "outputs": [],
   "source": [
    "from pandas import ExcelFile  # Nb. Need to install xlrd from conda (it does not automatically install with pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDjzKP6nNQtF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species_No</th>\n",
       "      <th>Petal_width</th>\n",
       "      <th>Petal_length</th>\n",
       "      <th>Sepal_width</th>\n",
       "      <th>Sepal_length</th>\n",
       "      <th>Species_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>7.4</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>7.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Species_No  Petal_width  Petal_length  Sepal_width  Sepal_length  \\\n",
       "0             1          0.2           1.4          3.5           5.1   \n",
       "1             1          0.2           1.4          3.0           4.9   \n",
       "2             1          0.2           1.3          3.2           4.7   \n",
       "3             1          0.2           1.5          3.1           4.6   \n",
       "4             1          0.2           1.4          3.6           5.0   \n",
       "5             1          0.4           1.7          3.9           5.4   \n",
       "6             1          0.3           1.4          3.4           4.6   \n",
       "7             1          0.2           1.5          3.4           5.0   \n",
       "8             1          0.2           1.4          2.9           4.4   \n",
       "9             1          0.1           1.5          3.1           4.9   \n",
       "10            1          0.2           1.5          3.7           5.4   \n",
       "11            1          0.2           1.6          3.4           4.8   \n",
       "12            1          0.1           1.4          3.0           4.8   \n",
       "13            1          0.1           1.1          3.0           4.3   \n",
       "14            1          0.2           1.2          4.0           5.8   \n",
       "15            1          0.4           1.5          4.4           5.7   \n",
       "16            1          0.4           1.3          3.9           5.4   \n",
       "17            1          0.3           1.4          3.5           5.1   \n",
       "18            1          0.3           1.7          3.8           5.7   \n",
       "19            1          0.3           1.5          3.8           5.1   \n",
       "20            1          0.2           1.7          3.4           5.4   \n",
       "21            1          0.4           1.5          3.7           5.1   \n",
       "22            1          0.2           1.0          3.6           4.6   \n",
       "23            1          0.5           1.7          3.3           5.1   \n",
       "24            1          0.2           1.9          3.4           4.8   \n",
       "25            1          0.2           1.6          3.0           5.0   \n",
       "26            1          0.4           1.6          3.4           5.0   \n",
       "27            1          0.2           1.5          3.5           5.2   \n",
       "28            1          0.2           1.4          3.4           5.2   \n",
       "29            1          0.2           1.6          3.2           4.7   \n",
       "..          ...          ...           ...          ...           ...   \n",
       "120           3          2.3           5.7          3.2           6.9   \n",
       "121           3          2.0           4.9          2.8           5.6   \n",
       "122           3          2.0           6.7          2.8           7.7   \n",
       "123           3          1.8           4.9          2.7           6.3   \n",
       "124           3          2.1           5.7          3.3           6.7   \n",
       "125           3          1.8           6.0          3.2           7.2   \n",
       "126           3          1.8           4.8          2.8           6.2   \n",
       "127           3          1.8           4.9          3.0           6.1   \n",
       "128           3          2.1           5.6          2.8           6.4   \n",
       "129           3          1.6           5.8          3.0           7.2   \n",
       "130           3          1.9           6.1          2.8           7.4   \n",
       "131           3          2.0           6.4          3.8           7.9   \n",
       "132           3          2.2           5.6          2.8           6.4   \n",
       "133           3          1.5           5.1          2.8           6.3   \n",
       "134           3          1.4           5.6          2.6           6.1   \n",
       "135           3          2.3           6.1          3.0           7.7   \n",
       "136           3          2.4           5.6          3.4           6.3   \n",
       "137           3          1.8           5.5          3.1           6.4   \n",
       "138           3          1.8           4.8          3.0           6.0   \n",
       "139           3          2.1           5.4          3.1           6.9   \n",
       "140           3          2.4           5.6          3.1           6.7   \n",
       "141           3          2.3           5.1          3.1           6.9   \n",
       "142           3          1.9           5.1          2.7           5.8   \n",
       "143           3          2.3           5.9          3.2           6.8   \n",
       "144           3          2.5           5.7          3.3           6.7   \n",
       "145           3          2.3           5.2          3.0           6.7   \n",
       "146           3          1.9           5.0          2.5           6.3   \n",
       "147           3          2.0           5.2          3.0           6.5   \n",
       "148           3          2.3           5.4          3.4           6.2   \n",
       "149           3          1.8           5.1          3.0           5.9   \n",
       "\n",
       "    Species_name  \n",
       "0         Setosa  \n",
       "1         Setosa  \n",
       "2         Setosa  \n",
       "3         Setosa  \n",
       "4         Setosa  \n",
       "5         Setosa  \n",
       "6         Setosa  \n",
       "7         Setosa  \n",
       "8         Setosa  \n",
       "9         Setosa  \n",
       "10        Setosa  \n",
       "11        Setosa  \n",
       "12        Setosa  \n",
       "13        Setosa  \n",
       "14        Setosa  \n",
       "15        Setosa  \n",
       "16        Setosa  \n",
       "17        Setosa  \n",
       "18        Setosa  \n",
       "19        Setosa  \n",
       "20        Setosa  \n",
       "21        Setosa  \n",
       "22        Setosa  \n",
       "23        Setosa  \n",
       "24        Setosa  \n",
       "25        Setosa  \n",
       "26        Setosa  \n",
       "27        Setosa  \n",
       "28        Setosa  \n",
       "29        Setosa  \n",
       "..           ...  \n",
       "120    Verginica  \n",
       "121    Verginica  \n",
       "122    Verginica  \n",
       "123    Verginica  \n",
       "124    Verginica  \n",
       "125    Verginica  \n",
       "126    Verginica  \n",
       "127    Verginica  \n",
       "128    Verginica  \n",
       "129    Verginica  \n",
       "130    Verginica  \n",
       "131    Verginica  \n",
       "132    Verginica  \n",
       "133    Verginica  \n",
       "134    Verginica  \n",
       "135    Verginica  \n",
       "136    Verginica  \n",
       "137    Verginica  \n",
       "138    Verginica  \n",
       "139    Verginica  \n",
       "140    Verginica  \n",
       "141    Verginica  \n",
       "142    Verginica  \n",
       "143    Verginica  \n",
       "144    Verginica  \n",
       "145    Verginica  \n",
       "146    Verginica  \n",
       "147    Verginica  \n",
       "148    Verginica  \n",
       "149    Verginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('DATA3/Iris.xls', sheet_name = 'Data', header = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0a2XhAZNQtH"
   },
   "source": [
    "So, this file appears to have an embedded table of aggregates on the same sheet as the raw data (a naughty but common practice amongst analysts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjWx-Xo0NQtH"
   },
   "source": [
    "It is usually better to load data correctly than to meddle with the source file or load it 'warts and all' and then try to parse it in code. The Pandas functions for reading files have parameters that provide the control we need. For ecxample, we could make multiple calls to `read_excel()`, using combinations of the `header`, `usecols`, `skiprows`, `nrows`, and `skipfooter` parameters to load one table at a time from a spreadsheet with multiple tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tC5kzTsMNQtI"
   },
   "source": [
    "Load the above file without the unwanted columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-P70uSXsNQtI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Petal_width</th>\n",
       "      <th>Petal_length</th>\n",
       "      <th>Sepal_length</th>\n",
       "      <th>Species_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>7.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2.5</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>Verginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Petal_width  Petal_length  Sepal_length Species_name\n",
       "0            0.2           1.4           5.1       Setosa\n",
       "1            0.2           1.4           4.9       Setosa\n",
       "2            0.2           1.3           4.7       Setosa\n",
       "3            0.2           1.5           4.6       Setosa\n",
       "4            0.2           1.4           5.0       Setosa\n",
       "5            0.4           1.7           5.4       Setosa\n",
       "6            0.3           1.4           4.6       Setosa\n",
       "7            0.2           1.5           5.0       Setosa\n",
       "8            0.2           1.4           4.4       Setosa\n",
       "9            0.1           1.5           4.9       Setosa\n",
       "10           0.2           1.5           5.4       Setosa\n",
       "11           0.2           1.6           4.8       Setosa\n",
       "12           0.1           1.4           4.8       Setosa\n",
       "13           0.1           1.1           4.3       Setosa\n",
       "14           0.2           1.2           5.8       Setosa\n",
       "15           0.4           1.5           5.7       Setosa\n",
       "16           0.4           1.3           5.4       Setosa\n",
       "17           0.3           1.4           5.1       Setosa\n",
       "18           0.3           1.7           5.7       Setosa\n",
       "19           0.3           1.5           5.1       Setosa\n",
       "20           0.2           1.7           5.4       Setosa\n",
       "21           0.4           1.5           5.1       Setosa\n",
       "22           0.2           1.0           4.6       Setosa\n",
       "23           0.5           1.7           5.1       Setosa\n",
       "24           0.2           1.9           4.8       Setosa\n",
       "25           0.2           1.6           5.0       Setosa\n",
       "26           0.4           1.6           5.0       Setosa\n",
       "27           0.2           1.5           5.2       Setosa\n",
       "28           0.2           1.4           5.2       Setosa\n",
       "29           0.2           1.6           4.7       Setosa\n",
       "..           ...           ...           ...          ...\n",
       "120          2.3           5.7           6.9    Verginica\n",
       "121          2.0           4.9           5.6    Verginica\n",
       "122          2.0           6.7           7.7    Verginica\n",
       "123          1.8           4.9           6.3    Verginica\n",
       "124          2.1           5.7           6.7    Verginica\n",
       "125          1.8           6.0           7.2    Verginica\n",
       "126          1.8           4.8           6.2    Verginica\n",
       "127          1.8           4.9           6.1    Verginica\n",
       "128          2.1           5.6           6.4    Verginica\n",
       "129          1.6           5.8           7.2    Verginica\n",
       "130          1.9           6.1           7.4    Verginica\n",
       "131          2.0           6.4           7.9    Verginica\n",
       "132          2.2           5.6           6.4    Verginica\n",
       "133          1.5           5.1           6.3    Verginica\n",
       "134          1.4           5.6           6.1    Verginica\n",
       "135          2.3           6.1           7.7    Verginica\n",
       "136          2.4           5.6           6.3    Verginica\n",
       "137          1.8           5.5           6.4    Verginica\n",
       "138          1.8           4.8           6.0    Verginica\n",
       "139          2.1           5.4           6.9    Verginica\n",
       "140          2.4           5.6           6.7    Verginica\n",
       "141          2.3           5.1           6.9    Verginica\n",
       "142          1.9           5.1           5.8    Verginica\n",
       "143          2.3           5.9           6.8    Verginica\n",
       "144          2.5           5.7           6.7    Verginica\n",
       "145          2.3           5.2           6.7    Verginica\n",
       "146          1.9           5.0           6.3    Verginica\n",
       "147          2.0           5.2           6.5    Verginica\n",
       "148          2.3           5.4           6.2    Verginica\n",
       "149          1.8           5.1           5.9    Verginica\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "df = pd.read_excel('DATA3/Iris.xls', sheet_name = 'Data', header = 0, usecols=[1,2,4,5])\n",
    "df\n",
    "#wanted to see if the usecols work, hence did not include columns 0,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTkOz1KsNQtK"
   },
   "source": [
    "### Importing Data Directly from the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFHi_S4fNQtK"
   },
   "source": [
    "We usually want to store a local copy of a data file that we download from the Web, but when data retention is not a priority it is convenient to download the data directly into our running Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jS7P3oXQNQtL"
   },
   "source": [
    "#### Importing Text Files from the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v-hzkxRRNQtL"
   },
   "source": [
    "The web is the 'wild west' of data formats. However, we can usually expect good behaviour from files that are automatically generated by a service, such as the earthquake report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFuaZ82hNQtM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>...</th>\n",
       "      <th>updated</th>\n",
       "      <th>place</th>\n",
       "      <th>type</th>\n",
       "      <th>horizontalError</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-15T07:23:48.470Z</td>\n",
       "      <td>18.8826</td>\n",
       "      <td>-66.420000</td>\n",
       "      <td>39.00</td>\n",
       "      <td>2.62</td>\n",
       "      <td>md</td>\n",
       "      <td>16</td>\n",
       "      <td>259</td>\n",
       "      <td>0.42000</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-07-15T07:41:08.120Z</td>\n",
       "      <td>46km N of Brenas, Puerto Rico</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.240</td>\n",
       "      <td>15</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>pr</td>\n",
       "      <td>pr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-15T07:14:01.150Z</td>\n",
       "      <td>35.9225</td>\n",
       "      <td>-117.683667</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.91</td>\n",
       "      <td>ml</td>\n",
       "      <td>24</td>\n",
       "      <td>51</td>\n",
       "      <td>0.03704</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-07-15T07:18:07.763Z</td>\n",
       "      <td>20km E of Little Lake, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.103</td>\n",
       "      <td>26</td>\n",
       "      <td>automatic</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time  latitude   longitude  depth   mag magType  nst  \\\n",
       "0  2019-07-15T07:23:48.470Z   18.8826  -66.420000  39.00  2.62      md   16   \n",
       "1  2019-07-15T07:14:01.150Z   35.9225 -117.683667   1.18  2.91      ml   24   \n",
       "\n",
       "   gap     dmin   rms  ...                   updated  \\\n",
       "0  259  0.42000  0.29  ...  2019-07-15T07:41:08.120Z   \n",
       "1   51  0.03704  0.20  ...  2019-07-15T07:18:07.763Z   \n",
       "\n",
       "                           place        type horizontalError depthError  \\\n",
       "0  46km N of Brenas, Puerto Rico  earthquake            1.15       3.10   \n",
       "1      20km E of Little Lake, CA  earthquake            0.28       0.53   \n",
       "\n",
       "   magError  magNst     status  locationSource magSource  \n",
       "0     0.240      15   reviewed              pr        pr  \n",
       "1     0.103      26  automatic              ci        ci  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_hour.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGgcXCzyNQtN"
   },
   "source": [
    "#### Importing HTML Files from the Web\n",
    "\n",
    "Working with unstructured HTML files relies heavily on library functions. This one, however, is well-structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4oVabZ6NQtO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                             Bank Name                City  \\\n",
       " 0                                 The Enloe State Bank              Cooper   \n",
       " 1                  Washington Federal Bank for Savings             Chicago   \n",
       " 2      The Farmers and Merchants State Bank of Argonia             Argonia   \n",
       " 3                                  Fayette County Bank          Saint Elmo   \n",
       " 4    Guaranty Bank, (d/b/a BestBank in Georgia & Mi...           Milwaukee   \n",
       " 5                                       First NBC Bank         New Orleans   \n",
       " 6                                        Proficio Bank  Cottonwood Heights   \n",
       " 7                        Seaway Bank and Trust Company             Chicago   \n",
       " 8                               Harvest Community Bank          Pennsville   \n",
       " 9                                          Allied Bank            Mulberry   \n",
       " 10                        The Woodbury Banking Company            Woodbury   \n",
       " 11                              First CornerStone Bank     King of Prussia   \n",
       " 12                                  Trust Company Bank             Memphis   \n",
       " 13                          North Milwaukee State Bank           Milwaukee   \n",
       " 14                              Hometown National Bank            Longview   \n",
       " 15                                 The Bank of Georgia      Peachtree City   \n",
       " 16                                        Premier Bank              Denver   \n",
       " 17                                      Edgebrook Bank             Chicago   \n",
       " 18                              Doral Bank  En Español            San Juan   \n",
       " 19                   Capitol City Bank & Trust Company             Atlanta   \n",
       " 20                             Highland Community Bank             Chicago   \n",
       " 21                    First National Bank of Crestview           Crestview   \n",
       " 22                                  Northern Star Bank             Mankato   \n",
       " 23              Frontier Bank, FSB D/B/A El Paseo Bank         Palm Desert   \n",
       " 24               The National Republic Bank of Chicago             Chicago   \n",
       " 25                                      NBRS Financial          Rising Sun   \n",
       " 26                               GreenChoice Bank, fsb             Chicago   \n",
       " 27                            Eastside Commercial Bank             Conyers   \n",
       " 28                              The Freedom State Bank             Freedom   \n",
       " 29                                         Valley Bank     Fort Lauderdale   \n",
       " ..                                                 ...                 ...   \n",
       " 526                                  ANB Financial, NA         Bentonville   \n",
       " 527                                          Hume Bank                Hume   \n",
       " 528                             Douglass National Bank         Kansas City   \n",
       " 529                                  Miami Valley Bank            Lakeview   \n",
       " 530                                            NetBank          Alpharetta   \n",
       " 531                          Metropolitan Savings Bank          Pittsburgh   \n",
       " 532                                    Bank of Ephraim             Ephraim   \n",
       " 533                                      Reliance Bank        White Plains   \n",
       " 534              Guaranty National Bank of Tallahassee         Tallahassee   \n",
       " 535                                Dollar Savings Bank              Newark   \n",
       " 536                               Pulaski Savings Bank        Philadelphia   \n",
       " 537              First National Bank of Blanchardville      Blanchardville   \n",
       " 538                              Southern Pacific Bank            Torrance   \n",
       " 539                        Farmers Bank of Cheneyville         Cheneyville   \n",
       " 540                                      Bank of Alamo               Alamo   \n",
       " 541             AmTrade International Bank  En Español             Atlanta   \n",
       " 542                     Universal Federal Savings Bank             Chicago   \n",
       " 543                       Connecticut Bank of Commerce            Stamford   \n",
       " 544                                   New Century Bank     Shelby Township   \n",
       " 545                              Net 1st National Bank          Boca Raton   \n",
       " 546                                       NextBank, NA             Phoenix   \n",
       " 547                           Oakwood Deposit Bank Co.             Oakwood   \n",
       " 548                              Bank of Sierra Blanca       Sierra Blanca   \n",
       " 549                      Hamilton Bank, NA  En Español               Miami   \n",
       " 550                             Sinclair National Bank            Gravette   \n",
       " 551                                 Superior Bank, FSB            Hinsdale   \n",
       " 552                                Malta National Bank               Malta   \n",
       " 553                    First Alliance Bank & Trust Co.          Manchester   \n",
       " 554                  National State Bank of Metropolis          Metropolis   \n",
       " 555                                   Bank of Honolulu            Honolulu   \n",
       " \n",
       "      ST   CERT                Acquiring Institution        Closing Date  \\\n",
       " 0    TX  10716                   Legend Bank, N. A.        May 31, 2019   \n",
       " 1    IL  30570                   Royal Savings Bank   December 15, 2017   \n",
       " 2    KS  17719                          Conway Bank    October 13, 2017   \n",
       " 3    IL   1802            United Fidelity Bank, fsb        May 26, 2017   \n",
       " 4    WI  30003  First-Citizens Bank & Trust Company         May 5, 2017   \n",
       " 5    LA  58302                         Whitney Bank      April 28, 2017   \n",
       " 6    UT  35495                    Cache Valley Bank       March 3, 2017   \n",
       " 7    IL  19328                  State Bank of Texas    January 27, 2017   \n",
       " 8    NJ  34951  First-Citizens Bank & Trust Company    January 13, 2017   \n",
       " 9    AR     91                         Today's Bank  September 23, 2016   \n",
       " 10   GA  11297                          United Bank     August 19, 2016   \n",
       " 11   PA  35312  First-Citizens Bank & Trust Company         May 6, 2016   \n",
       " 12   TN   9956           The Bank of Fayette County      April 29, 2016   \n",
       " 13   WI  20364  First-Citizens Bank & Trust Company      March 11, 2016   \n",
       " 14   WA  35156                       Twin City Bank     October 2, 2015   \n",
       " 15   GA  35259                        Fidelity Bank     October 2, 2015   \n",
       " 16   CO  34112            United Fidelity Bank, fsb       July 10, 2015   \n",
       " 17   IL  57772             Republic Bank of Chicago         May 8, 2015   \n",
       " 18   PR  32102         Banco Popular de Puerto Rico   February 27, 2015   \n",
       " 19   GA  33938  First-Citizens Bank & Trust Company   February 13, 2015   \n",
       " 20   IL  20290            United Fidelity Bank, fsb    January 23, 2015   \n",
       " 21   FL  17557                       First NBC Bank    January 16, 2015   \n",
       " 22   MN  34983                            BankVista   December 19, 2014   \n",
       " 23   CA  34738    Bank of Southern California, N.A.    November 7, 2014   \n",
       " 24   IL    916                  State Bank of Texas    October 24, 2014   \n",
       " 25   MD   4862                          Howard Bank    October 17, 2014   \n",
       " 26   IL  28462                 Providence Bank, LLC       July 25, 2014   \n",
       " 27   GA  58125            Community & Southern Bank       July 18, 2014   \n",
       " 28   OK  12483      Alva State Bank & Trust Company       June 27, 2014   \n",
       " 29   FL  21793  Landmark Bank, National Association       June 20, 2014   \n",
       " ..   ..    ...                                  ...                 ...   \n",
       " 526  AR  33901       Pulaski Bank and Trust Company         May 9, 2008   \n",
       " 527  MO   1971                        Security Bank       March 7, 2008   \n",
       " 528  MO  24660       Liberty Bank and Trust Company    January 25, 2008   \n",
       " 529  OH  16848         The Citizens Banking Company     October 4, 2007   \n",
       " 530  GA  32575                           ING DIRECT  September 28, 2007   \n",
       " 531  PA  35353  Allegheny Valley Bank of Pittsburgh    February 2, 2007   \n",
       " 532  UT   1249                        Far West Bank       June 25, 2004   \n",
       " 533  NY  26778                     Union State Bank      March 19, 2004   \n",
       " 534  FL  26838              Hancock Bank of Florida      March 12, 2004   \n",
       " 535  NJ  31330                          No Acquirer   February 14, 2004   \n",
       " 536  PA  27203                       Earthstar Bank   November 14, 2003   \n",
       " 537  WI  11639                        The Park Bank         May 9, 2003   \n",
       " 538  CA  27094                            Beal Bank    February 7, 2003   \n",
       " 539  LA  16445            Sabine State Bank & Trust   December 17, 2002   \n",
       " 540  TN   9961                          No Acquirer    November 8, 2002   \n",
       " 541  GA  33784                          No Acquirer  September 30, 2002   \n",
       " 542  IL  29355               Chicago Community Bank       June 27, 2002   \n",
       " 543  CT  19183                   Hudson United Bank       June 26, 2002   \n",
       " 544  MI  34979                          No Acquirer      March 28, 2002   \n",
       " 545  FL  26652                       Bank Leumi USA       March 1, 2002   \n",
       " 546  AZ  22314                          No Acquirer    February 7, 2002   \n",
       " 547  OH   8966       The State Bank & Trust Company    February 1, 2002   \n",
       " 548  TX  22002     The Security State Bank of Pecos    January 18, 2002   \n",
       " 549  FL  24382     Israel Discount Bank of New York    January 11, 2002   \n",
       " 550  AR  34248                   Delta Trust & Bank   September 7, 2001   \n",
       " 551  IL  32646                Superior Federal, FSB       July 27, 2001   \n",
       " 552  OH   6629                    North Valley Bank         May 3, 2001   \n",
       " 553  NH  34264  Southern New Hampshire Bank & Trust    February 2, 2001   \n",
       " 554  IL   3815              Banterra Bank of Marion   December 14, 2000   \n",
       " 555  HI  21029                   Bank of the Orient    October 13, 2000   \n",
       " \n",
       "            Updated Date  \n",
       " 0         June 18, 2019  \n",
       " 1      February 1, 2019  \n",
       " 2     February 21, 2018  \n",
       " 3      January 29, 2019  \n",
       " 4        March 22, 2018  \n",
       " 5      January 29, 2019  \n",
       " 6      January 29, 2019  \n",
       " 7      January 29, 2019  \n",
       " 8          May 18, 2017  \n",
       " 9          May 13, 2019  \n",
       " 10    December 13, 2018  \n",
       " 11    November 13, 2018  \n",
       " 12   September 14, 2018  \n",
       " 13     January 29, 2019  \n",
       " 14    February 19, 2018  \n",
       " 15         July 9, 2018  \n",
       " 16    February 20, 2018  \n",
       " 17     January 29, 2019  \n",
       " 18     January 29, 2019  \n",
       " 19     January 29, 2019  \n",
       " 20    November 15, 2017  \n",
       " 21    November 15, 2017  \n",
       " 22      January 3, 2018  \n",
       " 23    November 10, 2016  \n",
       " 24      January 6, 2016  \n",
       " 25     January 29, 2019  \n",
       " 26    December 12, 2016  \n",
       " 27      October 6, 2017  \n",
       " 28    February 21, 2018  \n",
       " 29     January 29, 2019  \n",
       " ..                  ...  \n",
       " 526    February 1, 2019  \n",
       " 527    January 31, 2019  \n",
       " 528    October 26, 2012  \n",
       " 529  September 12, 2016  \n",
       " 530    January 31, 2019  \n",
       " 531    October 27, 2010  \n",
       " 532       April 9, 2008  \n",
       " 533       April 9, 2008  \n",
       " 534      April 17, 2018  \n",
       " 535       April 9, 2008  \n",
       " 536     October 6, 2017  \n",
       " 537        June 5, 2012  \n",
       " 538    October 20, 2008  \n",
       " 539    October 20, 2004  \n",
       " 540      March 18, 2005  \n",
       " 541  September 11, 2006  \n",
       " 542     October 6, 2017  \n",
       " 543   February 14, 2012  \n",
       " 544      March 18, 2005  \n",
       " 545       April 9, 2008  \n",
       " 546    February 5, 2015  \n",
       " 547    October 25, 2012  \n",
       " 548    November 6, 2003  \n",
       " 549  September 21, 2015  \n",
       " 550     October 6, 2017  \n",
       " 551     August 19, 2014  \n",
       " 552   November 18, 2002  \n",
       " 553   February 18, 2003  \n",
       " 554      March 17, 2005  \n",
       " 555      March 17, 2005  \n",
       " \n",
       " [556 rows x 7 columns]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'\n",
    "df = pd.read_html(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaWhHAk9NQtQ"
   },
   "source": [
    "#### Importing XML Files from the Web\n",
    "\n",
    "XML files are semi-structured, but you're at the mercy of the file creator. If every record has the same format it will be much easier, but practical applications often require a lot of custom code. Here is an example that includes a nice parser class: http://www.austintaylor.io/lxml/python/pandas/xml/dataframe/2016/07/08/convert-xml-to-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppLnAmKVNQtQ"
   },
   "source": [
    "#### Importing JSON Files from the Web\n",
    "\n",
    "Like XML, JSON files are semi-structured and may require work to capture the schema into a dataframe. Here is a simple example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VB9EoRrNQtS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>2015-01-01 00:00:12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    integer            datetime  category\n",
       "0         5 2015-01-01 00:00:00         0\n",
       "1         5 2015-01-01 00:00:01         0\n",
       "10        5 2015-01-01 00:00:10         0\n",
       "11        5 2015-01-01 00:00:11         0\n",
       "12        8 2015-01-01 00:00:12         0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.json'\n",
    "\n",
    "# Load the first sheet of the JSON file into a data frame\n",
    "df = pd.read_json(url, orient = 'columns')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIArBrFFNQtV"
   },
   "source": [
    "## Part 2: Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aFUJrhENQtW"
   },
   "source": [
    "Data munging is manipulating data to get it into a form that we can start running analyses on (which usually means getting the data into a DataFrame). Before we get to this stage, we may need to remove headers or footers, transpose columns to rows, split wide data tables into long ones, and so on. (Nb. Excel files can be particularly troublesome, because users can format their data in mixed, complex shapes.) Essentially, we need to follow Hadley Wickham's guidelines for tidy datasets (http://vita.had.co.nz/papers/tidy-data.html):\n",
    "\n",
    "The end goal of the cleaning data process:\n",
    "\n",
    "- each variable should be in one column\n",
    "- each observation should comprise one row\n",
    "- each type of observational unit should form one table\n",
    "- include key columns for linking multiple tables\n",
    "- the top row contains (sensible) variable names\n",
    "- in general, save data as one file per table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kmjox61xNQtW"
   },
   "source": [
    "### Dataset Morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29PQdqUMNQtX"
   },
   "source": [
    "Once we have our dataset in a DataFrame (or Series, if our data is only 1-dimensional), we can start examining its size and content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04lspVeiNQtY"
   },
   "source": [
    "How many rows and columns are in `bikes`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DkO6SxSmNQtY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17379, 17)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "bikes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "camJWA-DNQta"
   },
   "source": [
    "What are the column names in `bikes`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDzIHgjXNQtb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
       "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
       "       'casual', 'registered', 'cnt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "bikes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iL7Cm_4NNQtc"
   },
   "source": [
    "What are the data types of these columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtKQtrdSNQtd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instant         int64\n",
       "dteday         object\n",
       "season          int64\n",
       "yr              int64\n",
       "mnth            int64\n",
       "hr              int64\n",
       "holiday         int64\n",
       "weekday         int64\n",
       "workingday      int64\n",
       "weathersit      int64\n",
       "temp          float64\n",
       "atemp         float64\n",
       "hum           float64\n",
       "windspeed     float64\n",
       "casual          int64\n",
       "registered      int64\n",
       "cnt             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "bikes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqHDi7_GNQtf"
   },
   "source": [
    "What is the (row) index for this DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7WciAwrNQtf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=17379, step=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "bikes.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIJcLnsKNQth"
   },
   "source": [
    "https://www.dataquest.io/blog/python-json-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtxBEejpNQti"
   },
   "source": [
    "## Slicing and Dicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tzns_pFsNQtj"
   },
   "source": [
    "It is often preferable to refer to DataFrame columns by name, but there is more than one way to do this. \n",
    "Do `bikes['season']` and `bikes[['season']]` give the same object? Demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSlC2oZXNQtj"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsCATb2iNQtl"
   },
   "source": [
    "How would we use object notation to show the first 4 rows of `atemp`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5D_UU5KNQtm"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWKI9FNDNQto"
   },
   "source": [
    "Algorithms that loop over multiple columns often access DataFrame columns by index. However, none of the following work (try them out by uncommenting / removing the \"#E: \" ): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NgWFEEmWNQto"
   },
   "outputs": [],
   "source": [
    "bikes[[0]]\n",
    "#E: bikes[0]\n",
    "#E: bikes[0,0]\n",
    "#E: bikes[[0,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeUJ7D5qNQtq"
   },
   "source": [
    "What is the correct way to access the 1st row of the DataFrame by its index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4Kidzz0NQtq"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZa1v-2jNQts"
   },
   "source": [
    "What is the correct way to access the 2nd column of the DataFrame by its index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4GmE0EsNQtt"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aSvqNbVUNQtu"
   },
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRPFEN1HNQtu"
   },
   "source": [
    "What is the Pandas `isnull` function for? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xyw5qkWWNQtu"
   },
   "source": [
    "?\n",
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iby8s2VSNQtv"
   },
   "source": [
    "We can apply `isnull` to the `bikes` DataFrame to show the result for every element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRQY-1ViNQtv"
   },
   "outputs": [],
   "source": [
    "bikes.isnull().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IyZaICINQtw"
   },
   "source": [
    "However, we usually start at a higher level. How many nulls are in `bikes` altogether?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbDfiSqVNQtx"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10jWUf4VNQty"
   },
   "source": [
    "If this result were nonzero we would next want to find out which columns contained nulls. How can this be done in one line of code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBv3l_s2NQtz"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1MsvXf7NQt0"
   },
   "source": [
    "What is the Numpy object `nan` used for? (Write a descriptive answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GaeGVh6ZNQt0"
   },
   "source": [
    "?\n",
    "ANSWER: Marking a data point as invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9bFlPsrNQt1"
   },
   "source": [
    "Write (and verify) a function that performs scalar division with built-in handling of the edge case (i.e. return a value instead of just trapping the error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Cq2VAb8NQt1"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7Up8D6lNQt2"
   },
   "source": [
    "Apply the Pandas `isna` function to the following data objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_YvVav3NQt3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 2.3\n",
    "y = np.nan\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAcf1FU1NQt4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJUM31pANQt5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array = np.array([[1, np.nan, 3], [4, 5, np.nan]])\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LBgqnubNQt6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhFaZbzQNQt7"
   },
   "source": [
    "How is the pandas I/O parameter `na_values` used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mw-PvrTeNQt8"
   },
   "source": [
    "? ANSWER: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOW3ICgwNQt8"
   },
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZhJ9-XrNQt8"
   },
   "source": [
    "### Counts\n",
    "\n",
    "When there are categorical variables in a dataset we will want to know how many possible values there are in each column. (Nb. If the dataset is a sample of a larger one, our sample may not capture all possible values of every categorical.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RitKNRPCNQt8"
   },
   "source": [
    "How many (different) seasons are in `bikes`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqmE4OMsNQt9"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peDZrNJjNQt-"
   },
   "source": [
    "### Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIKyD5LHNQt-"
   },
   "source": [
    "Print the range of the `instant`, `dteday`, and `windspeed` columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVAOjyocNQt-"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5oDTJHoNQt_"
   },
   "source": [
    "Compute and print the overall minimum and maximum of the numeric data columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiIKiPT4NQuA"
   },
   "outputs": [],
   "source": [
    "bikes_min, bikes_max = (min(bikes.min()), max(bikes.max()))\n",
    "bikes_min, bikes_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OyKKjxJoNQuB"
   },
   "source": [
    "### Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hxPA3sXNQuB"
   },
   "source": [
    "Pandas makes computing quantiles easy. This is how to get the median of a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1GLqWOoNQuB"
   },
   "outputs": [],
   "source": [
    "bikes['atemp'].quantile(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBWCtrCrNQuD"
   },
   "source": [
    "Of course, the `quantiles` method can take a tuple as its argument. Compute the 10th, 25th, 50th, 75th, and 90th percentiles in one line of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iu8bzEktNQuD"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EJnCB_bqNQuF"
   },
   "source": [
    "### Cuts\n",
    "\n",
    "Sometimes we want to split the sample not by the quantiles of the distribution but by the range of the data. Let's take a closer look at `atemp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vn_cvvY-NQuF"
   },
   "outputs": [],
   "source": [
    "type(bikes['atemp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ae7zUQp-NQuH"
   },
   "outputs": [],
   "source": [
    "bikes.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thwFHrthNQuJ"
   },
   "source": [
    "Suppose we decide to sort these values into 4 bins of equal width, but we want to apply the resulting groups to the entire DataFrame. Basically, we need to add a row label that indcates which bin each sample belongs in. Let's call this label \"atemp_level\", and use the `cut` method to populate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7mXBeXMNQuJ"
   },
   "outputs": [],
   "source": [
    "atemp_level = pd.cut(bikes['atemp'], bins = 4)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vf3Q5vbQNQuL"
   },
   "source": [
    "What is `atemp_level`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhlx1W-VNQuL"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GuDXdgfxNQuN"
   },
   "source": [
    "Here is a random sample of `atemp_level`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Qure0UbNQuN"
   },
   "outputs": [],
   "source": [
    "atemp_level.sample(5)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q59qWmqfNQuO"
   },
   "source": [
    "So, by default, `cut` produces labels that indicate the bin boundaries for each element in the series it was applied to. Usually, we will specify labels that are appropriate to the discretisation we are applying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYsD8ZwDNQuP"
   },
   "outputs": [],
   "source": [
    "atemp_level = pd.cut(bikes['atemp'], bins = 4, labels = [\"cool\", \"mild\", \"warm\", \"hot\"])\n",
    "atemp_level.sample(5)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WD-3g9qLNQuQ"
   },
   "source": [
    "Incorporate the new `atemp_level` column into the `bikes` DataFrame and use it to count the number of \"mild\" `atemp` entries in `season` 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5fRRbXwNQuR"
   },
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZN-0yDQNQuR"
   },
   "source": [
    "*Nb. The `atemp_level` variable we created is what the R language calls a \"factor\". Pandas has introduced a new data type called \"category\" that is similar to R's factors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxMDzgRWNQuS"
   },
   "source": [
    "# Synthetic Data\n",
    "\n",
    "Sometimes we may want to generate test data, or we may need to initalise a series, matrix, or data frame for input to an algorithm. Numpy has several methods we can use for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_W10uoBlNQuS"
   },
   "source": [
    "Execute the following, then check the shape and content of each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFR29OriNQuS"
   },
   "outputs": [],
   "source": [
    "# Creating arrays with initial values\n",
    "a = np.zeros((3))\n",
    "b = np.ones((1,3))\n",
    "c = np.random.randint(1,10,(2,3,4))   # randint(low, high, size)\n",
    "d = np.arange(4)\n",
    "e = np.array( [[1,2,3,4], [5,6,7,8]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGrvvNNANQuT"
   },
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load rock.csv and clean the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Column Names\n",
    "\n",
    "Check column names and clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Null Values With 0\n",
    "\n",
    "Check 'release' column whether this column have any null value or not. Replace null value with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Datatypes of Dataset\n",
    "\n",
    "Check datatypes of the dataset. Is there any column which should be int instead of object? Fix the column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Min, Max of Each Column\n",
    "\n",
    "Is there any illogical value in any column? How can we fix that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Some Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function that will take a row of a DataFrame and print out the song, artist, and whether or not the release date is < 1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function that converts cells in a DataFrame to float and otherwise replaces them with np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply these functions to your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the new float-only DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hj8GdoOXC-Lu"
   },
   "source": [
    "\n",
    "\n",
    "> \n",
    ">\n",
    ">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YLHUmjnDDZq"
   },
   "source": [
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Nov8gGCDExO"
   },
   "source": [
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2asO3_lDFm8"
   },
   "source": [
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlasiTKgDGdA"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> > > > > > > >   © 2019 Data Science Institute of Australia\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Q0G5PtA2NQtC",
    "pTkOz1KsNQtK",
    "jS7P3oXQNQtL",
    "kGgcXCzyNQtN",
    "CaWhHAk9NQtQ",
    "ppLnAmKVNQtQ",
    "Kmjox61xNQtW",
    "JZhJ9-XrNQt8",
    "peDZrNJjNQt-",
    "OyKKjxJoNQuB",
    "EJnCB_bqNQuF"
   ],
   "name": "DSIA Lab 3.1.1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
